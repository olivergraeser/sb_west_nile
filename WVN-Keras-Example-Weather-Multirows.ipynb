{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import date, datetime\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from utils import read_basic_dataset\n",
    "\n",
    "from features import get_month_virus_share, create_trap_distance_matrix, get_nearest_trap, get_nearest_trap_list, \\\n",
    "    add_multirows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold_count = 5\n",
    "seed = 1337\n",
    "train_verbose = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Basic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_features, training_target, test_features = read_basic_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trap_records = pd.concat([training_features[['Trap', 'Latitude', 'Longitude']], \n",
    "                          test_features[['Trap', 'Latitude', 'Longitude']]])\n",
    "trap_distance_matrix = create_trap_distance_matrix(trap_records)\n",
    "#trap_distance_matrix.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_features = add_multirows(training_features)\n",
    "test_features = add_multirows(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_features = ['week', 'Latitude', 'Longitude', \n",
    "                 'Tmax', 'Tmin', 'Tavg', 'DewPoint', 'StnPressure', 'PrecipTotal',\n",
    "                 '', '10dtmin_min', '10dtavg_avg', '10dpcp_tot', '10ddwp_avg', '10dprs_avg']\n",
    "drop_features = ['AddressAccuracy','AddressNumberAndStreet','Address', 'Block', 'Date', \n",
    "                 'Heat', 'Cool', 'Sunrise', 'Sunset','Depth','Water1','SeaLevel', 'SnowFall', 'CodeSum', \n",
    "                 'Depart', 'WetBulb', 'ResultSpeed', 'ResultDir', 'AvgSpeed', \n",
    "                 'month', 'Species', 'station','Street', 'Trap', 'Station', 'ddate', \n",
    "                 #'10dtmax_max',\n",
    "                 '10dtmax_min', \n",
    "                 '10dtmax_avg', \n",
    "                 '10dtavg_max', \n",
    "                 #'10dtavg_avg', \n",
    "                 #'10dtavg_min', \n",
    "                 '10dtmin_max', \n",
    "                 '10dtmin_avg',\n",
    "                 #'10dtmin_min', \n",
    "                 #'10dpcp_tot', \n",
    "                 #'10ddwp_avg', \n",
    "                 #'10dprs_avg'\n",
    "                ]\n",
    "\n",
    "training_features_input = training_features.drop(drop_features + ['NumMosquitos', 'WnvPresent'], axis=1)\n",
    "test_features_input = test_features.drop(drop_features +['Id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "shuffle = np.arange(len(training_features_input))\n",
    "np.random.shuffle(shuffle)\n",
    "training_target_input = training_target.iloc[shuffle]\n",
    "training_features_input = training_features_input.iloc[shuffle]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(training_features_input)\n",
    "training_feature_array = scaler.transform(training_features_input)\n",
    "training_target_array = np.asarray(training_target_input)\n",
    "test_feature_array = scaler.transform(test_features_input.fillna(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(fold_cnt, feature_array, target_array, model_generator, fitting_function):\n",
    "    folds = KFold(len(target_array), fold_count)\n",
    "    mean_auroc_valid = 0.\n",
    "    mean_auroc_train = 0\n",
    "    target_array_categorical = np_utils.to_categorical(target_array)\n",
    "    trained_models = list()\n",
    "    for i, (train, valid) in enumerate(folds):\n",
    "        print('Fold', i)\n",
    "        X_train = feature_array[train]\n",
    "        X_valid = feature_array[valid]\n",
    "        Y_train = target_array_categorical[train]\n",
    "        y_train = target_array[train]\n",
    "        Y_valid = target_array_categorical[valid]\n",
    "        y_valid = target_array[valid]\n",
    "        foldmodel = model_generator()\n",
    "        train_and_valid_data = (X_train, Y_train, y_train, X_valid, Y_valid, y_valid)\n",
    "        fitting_function(foldmodel, train_and_valid_data)\n",
    "        trained_models.append(foldmodel)\n",
    "        valid_preds = foldmodel.predict_proba(X_valid)\n",
    "        training_preds = foldmodel.predict_proba(X_train)\n",
    "        roc_valid = metrics.roc_auc_score(y_valid, valid_preds[:, 1])\n",
    "        roc_train = metrics.roc_auc_score(y_train, training_preds[:, 1])\n",
    "        #print(\"ROC: {} training, {} validation\".format(roc_train, roc_valid))\n",
    "        mean_auroc_train += roc_train\n",
    "        mean_auroc_valid += roc_valid\n",
    "            \n",
    "    print('Average ROC:', mean_auroc_train/fold_count, mean_auroc_valid/fold_count)\n",
    "    return trained_models\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb = XGBClassifier()\n",
    "#xgb.fit(training_feature_array, training_target_array.ravel())\n",
    "#xgb.predict_proba(np.array(test_feature_array))\n",
    "\n",
    "def xgb_model_builder(xgb_model_dict=None):\n",
    "    if not xgb_model_dict:\n",
    "        xgb_model_dict={'n_estimators': 200,\n",
    "                        'max_depth': 4,\n",
    "                        'reg_alpha':0.01,\n",
    "                        'seed':seed}\n",
    "    return XGBClassifier(**xgb_model_dict)\n",
    "\n",
    "def xgb_fitting_function(xgb_model, tvd):\n",
    "    xgb_model.fit(tvd[0], tvd[2].ravel())\n",
    "    \n",
    "xgbms = train_model(fold_count, training_feature_array, training_target_array, xgb_model_builder, xgb_fitting_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_prediction_array = [xgb.predict_proba(np.nan_to_num(test_feature_array))[:,1] \n",
    "                        for xgb in xgbms]\n",
    "bagged_xgb_prediction = np.mean(np.array(xgb_prediction_array), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'loss': 'categorical_crossentropy',\n",
    "    'optimizer': 'adadelta',\n",
    "    'layers': [{'nodecount': 20, 'activation': 'relu', 'dropout': 0.5},\n",
    "               {'nodecount': 10, 'activation': 'relu', 'dropout': 0.25},\n",
    "               {'nodecount': 5, 'activation': 'relu', 'dropout': 0.125}],\n",
    "    'dimension_out': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_dict):\n",
    "    model = Sequential()\n",
    "    input_dim = model_dict['dimension_input']\n",
    "    for layer in model_dict['layers']:\n",
    "        model.add(Dense(layer['nodecount'], input_dim=input_dim))\n",
    "        model.add(Activation(layer['activation']))\n",
    "        model.add(Dropout(layer['dropout']))\n",
    "        input_dim = layer['nodecount']\n",
    "\n",
    "    model.add(Dense(model_dict['dimension_output']))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=model_dict['loss'], optimizer=model_dict['optimizer'])\n",
    "    return model\n",
    "\n",
    "model_dict['dimension_input'] = training_feature_array.shape[1]\n",
    "model_dict['dimension_output'] = len(np.unique(training_target_array))\n",
    "\n",
    "def keras_model_builder():\n",
    "    return build_model(model_dict)\n",
    "def keras_fit(keras_model, tvd):\n",
    "    keras_model.fit(tvd[0], tvd[1], epochs=50, batch_size=32, validation_data=(tvd[3], tvd[4]), verbose=train_verbose)\n",
    "\n",
    "fnn = keras_model_builder()\n",
    "fnnms = train_model(fold_count, training_feature_array, training_target_array, keras_model_builder, keras_fit)\n",
    "#keras_fit(fnn, )\n",
    "#FNN = train_model(4, train_init_array, train_target_id, keras_model_builder, keras_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_prediction_array = [fnn.predict_proba(np.nan_to_num(test_feature_array))[:,1] for fnn in fnnms]\n",
    "bagged_fnn_prediction = np.mean(np.array(fnn_prediction_array), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features['WnvPresent'] = bagged_xgb_prediction\n",
    "export_df = test_features[['Id', 'WnvPresent']]\n",
    "export_df.to_csv('multirow_only.csv', index=False , quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
