{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import date, datetime\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "from utils import read_basic_dataset\n",
    "\n",
    "from features import get_month_virus_share, create_trap_distance_matrix, get_nearest_trap, get_nearest_trap_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fold_count = 5\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Basic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_features, training_target, test_features = read_basic_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trap_records = pd.concat([training_features[['Trap', 'Latitude', 'Longitude']], \n",
    "                          test_features[['Trap', 'Latitude', 'Longitude']]])\n",
    "trap_distance_matrix = create_trap_distance_matrix(trap_records)\n",
    "#trap_distance_matrix.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "virus_per_trap_species, virus_per_trap, virus_per_species_overall = get_month_virus_share(training_features)\n",
    "training_features = pd.merge(how='left', left=training_features, right=virus_per_trap_species, \n",
    "                             left_on=['month', 'Trap', 'Species'], right_index=True)\n",
    "training_features['virusshare'] = training_features.apply(lambda _: _['virusshare_ts'], axis=1)\n",
    "training_features['species_virus'] = training_features.Species.\\\n",
    "    apply(lambda _: virus_per_species_overall.loc[_]['virusshare_s'] if _ in virus_per_species_overall.index else -1)\n",
    "#cols_to_use = test_features.columns.difference(virus_share_df.columns)\n",
    "test_features = pd.merge(how='left', left=test_features, right=virus_per_trap_species, \n",
    "                             left_on=['month', 'Trap', 'Species'], right_index=True)\n",
    "test_features = pd.merge(how='left', left=test_features, right=virus_per_trap, \n",
    "                             left_on=['month', 'Trap'], right_index=True)\n",
    "test_features['virusshare'] = -1\n",
    "test_features['location_distance'] = -1\n",
    "test_features['species_virus'] = test_features.Species.\\\n",
    "    apply(lambda _: virus_per_species_overall.loc[_]['virusshare_s'] \n",
    "          if _ in virus_per_species_overall.index else -1)\n",
    "#test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in test_features.iterrows():\n",
    "    print(i)\n",
    "    #import pdb; pdb.set_trace()\n",
    "    row = row.copy()\n",
    "    if not pd.isnull(row['virusshare_ts']):\n",
    "        row['virusshare'] = row['virusshare_ts']\n",
    "        row['location_distance'] = 0\n",
    "    elif not pd.isnull(row['virusshare_t']):\n",
    "        row['virusshare'] = row['virusshare_t']\n",
    "        row['location_distance'] = 0    \n",
    "    else:\n",
    "        #import pdb; pdb.set_trace()\n",
    "        station_list = get_nearest_trap_list(trap_distance_matrix, row['Trap'])\n",
    "        for j, s in station_list.iterrows():\n",
    "            if (row['month'], j) in virus_per_trap.index:\n",
    "                row['virusshare'] = virus_per_trap.loc[row['month'], j]['virusshare_t']\n",
    "                row['location_distance'] = s['distance'] \n",
    "    test_features.loc[i, 'virusshare'] = row['virusshare']\n",
    "    test_features.loc[i, 'location_distance'] = row['location_distance']\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features.to_pickle('test_features_virus.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = pd.read_pickle('test_features_virus.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_features = ['month', 'week', 'Latitude', 'Longitude', \n",
    "                 'Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb', 'StnPressure',\n",
    "                 'virusshare', 'species_virus']\n",
    "training_features_input = training_features[keep_features]\n",
    "test_features_input = test_features[keep_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "shuffle = np.arange(len(training_features_input))\n",
    "np.random.shuffle(shuffle)\n",
    "training_target_input = training_target.iloc[shuffle]\n",
    "training_features_input = training_features_input.iloc[shuffle]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(training_features_input)\n",
    "training_feature_array = scaler.transform(training_features_input)\n",
    "training_target_array = np.asarray(training_target_input)\n",
    "test_feature_array = scaler.transform(test_features_input.fillna(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(fold_cnt, feature_array, target_array, model_generator, fitting_function):\n",
    "    folds = KFold(len(target_array), fold_count)\n",
    "    mean_auroc_valid = 0.\n",
    "    mean_auroc_train = 0\n",
    "    target_array_categorical = np_utils.to_categorical(target_array)\n",
    "    trained_models = list()\n",
    "    for i, (train, valid) in enumerate(folds):\n",
    "        print('---'*20)\n",
    "        print('Fold', i)\n",
    "        print('---'*20)\n",
    "        X_train = feature_array[train]\n",
    "        X_valid = feature_array[valid]\n",
    "        Y_train = target_array_categorical[train]\n",
    "        y_train = target_array[train]\n",
    "        Y_valid = target_array_categorical[valid]\n",
    "        y_valid = target_array[valid]\n",
    "\n",
    "        print(\"Building model...\")\n",
    "        foldmodel = model_generator()\n",
    "        print(\"Training model...\")\n",
    "        train_and_valid_data = (X_train, Y_train, y_train, X_valid, Y_valid, y_valid)\n",
    "        fitting_function(foldmodel, train_and_valid_data)\n",
    "        trained_models.append(foldmodel)\n",
    "        #foldmodel.fit(, epochs=100, batch_size=16, validation_data=(), verbose=0)\n",
    "        valid_preds = foldmodel.predict_proba(X_valid)\n",
    "        training_preds = foldmodel.predict_proba(X_train)\n",
    "        roc_valid = metrics.roc_auc_score(y_valid, valid_preds[:, 1])\n",
    "        roc_train = metrics.roc_auc_score(y_train, training_preds[:, 1])\n",
    "        print(\"ROC: {} training, {} validation\".format(roc_train, roc_valid))\n",
    "        mean_auroc_train += roc_train\n",
    "        mean_auroc_valid += roc_valid\n",
    "            \n",
    "    print('Average ROC:', mean_auroc_train/fold_count, mean_auroc_valid/fold_count)\n",
    "    return trained_models\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb = XGBClassifier()\n",
    "#xgb.fit(training_feature_array, training_target_array.ravel())\n",
    "#xgb.predict_proba(np.array(test_feature_array))\n",
    "\n",
    "def xgb_model_builder(xgb_model_dict=None):\n",
    "    if not xgb_model_dict:\n",
    "        xgb_model_dict=dict()\n",
    "    return XGBClassifier(**xgb_model_dict)\n",
    "\n",
    "def xgb_fitting_function(xgb_model, tvd):\n",
    "    xgb_model.fit(tvd[0], tvd[2].ravel())\n",
    "    \n",
    "tvd = train_model(fold_count, training_feature_array, training_target_array, xgb_model_builder, xgb_fitting_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = xgb_model_builder()\n",
    "xgb_fitting_function(xgb, (training_feature_array,None, training_target_array))\n",
    "preds = xgb.predict_proba(np.nan_to_num(test_feature_array))\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    'loss': 'categorical_crossentropy',\n",
    "    'optimizer': 'adadelta',\n",
    "    'layers': [{'nodecount': 20, 'activation': 'relu', 'dropout': 0.5},\n",
    "               {'nodecount': 10, 'activation': 'relu', 'dropout': 0.25},\n",
    "               {'nodecount': 5, 'activation': 'relu', 'dropout': 0.125}],\n",
    "    'dimension_out': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(model_dict):\n",
    "    model = Sequential()\n",
    "    input_dim = model_dict['dimension_input']\n",
    "    for layer in model_dict['layers']:\n",
    "        model.add(Dense(layer['nodecount'], input_dim=input_dim))\n",
    "        model.add(Activation(layer['activation']))\n",
    "        model.add(Dropout(layer['dropout']))\n",
    "        input_dim = layer['nodecount']\n",
    "\n",
    "    model.add(Dense(model_dict['dimension_output']))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss=model_dict['loss'], optimizer=model_dict['optimizer'])\n",
    "    return model\n",
    "\n",
    "model_dict['dimension_input'] = training_feature_array.shape[1]\n",
    "model_dict['dimension_output'] = len(np.unique(training_target_array))\n",
    "\n",
    "def keras_model_builder():\n",
    "    return build_model(model_dict)\n",
    "def keras_fit(keras_model, tvd):\n",
    "    keras_model.fit(tvd[0], tvd[1], epochs=50, batch_size=32, validation_data=(tvd[3], tvd[4]), verbose=1)\n",
    "\n",
    "fnn = keras_model_builder()\n",
    "tvd = train_model(fold_count, training_feature_array, training_target_array, keras_model_builder, keras_fit)\n",
    "#keras_fit(fnn, )\n",
    "#FNN = train_model(4, train_init_array, train_target_id, keras_model_builder, keras_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_array = [fnn.predict_proba(np.nan_to_num(test_feature_array))[:,1] for fnn in tvd]\n",
    "bagged_prediction = np.mean(np.array(prediction_array), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features['WnvPresent'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_df = test_features[['Id', 'WnvPresent']]\n",
    "export_df.to_csv('submission_08.csv', index=False , quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
